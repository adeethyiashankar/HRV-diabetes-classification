{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Preprocess HMP Data.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9byeh9JIpQwj","executionInfo":{"status":"ok","timestamp":1631376541221,"user_tz":240,"elapsed":320,"user":{"displayName":"Adeethyia Shankar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13953621006807715822"}},"outputId":"9642e1e4-a65e-465b-e197-89867456b500"},"source":["from scipy import stats\n","import pandas as pd\n","import numpy as np\n","import math\n","import os\n","import multiprocessing\n","import progressbar\n","from time import time\n","\n","data_path = 'gdrive/My Drive/Summer Research/hmp2-data-stanford/'\n","preprocessed_data_path = 'gdrive/My Drive/Summer Research/hmp2-data-stanford/Preprocessed/'\n","wt_data_path = 'gdrive/My Drive/Summer Research/hmp2-data-stanford/Preprocessed/Wavelet Transform/'\n","paths = [preprocessed_data_path, wt_data_path+'Denoised/',\n","         wt_data_path+'WT Domain/']\n","\n","hmp_datas = ['cytokine_abundance','gut_16s_abundance','Lipidomics',\n","          'metabolome_abundance','Metabolomics','nares_16s_abundance',\n","          'proteome_abundance','Proteomics','RNAseq_abundance',\n","          'Targ.proteomics','Transcriptomics_VST_excl_3participants']\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"KtOJryF-qK5o"},"source":["def get_subjects():\n","  df = pd.read_csv(data_path+'Subjects.csv', index_col=False)\n","  subjects = dict()\n","  for i in range(len(df)):\n","    subject_data = dict(df.iloc[i,:])\n","    subject_id = subject_data.pop('SubjectID')\n","    subjects[subject_id] = subject_data\n","  return subjects"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"COkn9lIe0n9v"},"source":["def get_class_from_visit_id():\n","  df = pd.read_csv(data_path+'Visit.csv', index_col=False,\n","                   usecols=['VisitID', 'SubjectID'])\n","  subjects = get_subjects()\n","  return dict((df.iloc[i,0],subjects[df.iloc[i,1]]) for i in range(len(df)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1y2Fa_IkrATJ"},"source":["def get_data(f):\n","  df = pd.read_csv(data_path+f+'.csv', index_col=False)\n","  gc = get_class_from_visit_id()\n","  race = list()\n","  sex = list()\n","  age = list()\n","  bmi = list()\n","  sspg = list()\n","  classifications = list()\n","  indices_to_drop = list()\n","  nan_values = [i for i in range(len(df)) if df.isnull().any(axis=1)[i]]\n","\n","  bools = list(df.astype('bool').mean(axis=1) < 0.75)\n","  zero_values = [i for i in range(len(bools)) if bools[i] == True]\n","  nan_values.extend(zero_values)\n","\n","  for i in range(len(df)):\n","    v_id = df.iloc[i,0]\n","    try:\n","      if i not in nan_values:\n","        c = gc[v_id]\n","        race.append(c['Race'])\n","        sex.append(c['Sex'])\n","        age.append(c['Age'])\n","        bmi.append(c['BMI'])\n","        sspg.append(c['SSPG'])\n","        classifications.append(c['IR_IS_classification'])\n","      else:\n","        indices_to_drop.append(i)\n","    except KeyError:\n","      indices_to_drop.append(i)\n","  \n","  df = df.drop(indices_to_drop)\n","  df.insert(1, 'IR_IS_classification', classifications, False)\n","  df.insert(1, 'SSPG', sspg, False)\n","  df.insert(1, 'BMI', bmi, False)\n","  df.insert(1, 'Age', age, False)\n","  df.insert(1, 'Sex', sex, False)\n","  df.insert(1, 'Race', race, False)\n","  return df.reset_index(drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OECUhtTNCeBp"},"source":["def save_all_data():\n","  for i in hmp_datas:\n","    df = get_data(i)\n","    df.to_csv(preprocessed_data_path+i+'.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0K4Z0FWkJkHs"},"source":["#save_all_data()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aY30hCGKL3qs"},"source":["def check_for_nan():\n","  for i in hmp_datas:\n","    df = get_data(i)\n","    print(i+': '+str(df.isnull().values.sum()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cm9cwBoUSfKU"},"source":["def matrix_size():\n","  for i in hmp_datas:\n","      df = pd.read_csv(preprocessed_data_path+i+'.csv', index_col=False)\n","      print(i+': '+str(df.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O9Hn_-hNxdJv"},"source":["Normalization"]},{"cell_type":"code","metadata":{"id":"JXM7547hxeH4"},"source":["def normalization():\n","  widgets = [' [',\n","        progressbar.Timer(format= 'elapsed time: %(elapsed)s'),\n","        '] ',\n","          progressbar.Bar('#'),' (',\n","          progressbar.ETA(), ') ',\n","          progressbar.Counter(format='%(value)d/%(max_value)d')\n","          ]\n","  bar = progressbar.ProgressBar(max_value=len(hmp_datas)-2, widgets=widgets).start()\n","  count = 0\n","\n","  for j in hmp_datas:\n","    if j not in ['metabolome_abundance', 'proteome_abundance']:\n","      count += 1\n","      bar.update(count)\n","        \n","      sid_and_class = pd.read_csv(preprocessed_data_path+j+'.csv', index_col=False,\n","                        usecols=['SampleID', 'Race', 'Sex', 'Age', 'BMI', 'SSPG', 'IR_IS_classification'])\n","      df = pd.read_csv(preprocessed_data_path+j+'.csv', index_col=False).drop(['SampleID', 'Race', 'Sex', 'Age', 'BMI', 'SSPG', 'IR_IS_classification'], axis=1)\n","      cols = ['SampleID', 'Race', 'Sex', 'Age', 'BMI', 'SSPG', 'IR_IS_classification'] + list(df.columns)\n","\n","      add = 0.1\n","      df += add\n","      df = np.log2(df)\n","      df = pd.concat([sid_and_class, df], axis=1, ignore_index=True)\n","      df.columns = pd.Index(cols)\n","\n","      df.to_csv(preprocessed_data_path+'Normalized/'+j+'.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8zWtp-RtP65g"},"source":["Wavelet Transform"]},{"cell_type":"code","metadata":{"id":"TE_y2lMgQECq"},"source":["def fourWTM(n):\n","  #Filter banks\n","  h0 = np.array([0.2697890,0.3947890,0.5197890,0.6447890,0.2302110,0.1052110,-0.0197890,-0.1447890])\n","  h1 = np.array([-0.2825435,0.5553379,0.2385187,-0.0783004, -0.5834819,-0.2666627,0.0501564,0.3669755])\n","  h2 = np.array([0.4125840,-0.6279376,0.3727824,0.1487574, -0.4125840,-0.1885590,0.0354659,0.2594909])\n","  h3 = np.array([0.2382055,0.1088646,-0.7275830,0.5572896, -0.2382055,-0.1088646,0.0204763,0.1498171])\n","  #Matrix of filter banks created for convenience\n","  h = np.array([h0,h1,h2,h3])\n","\n","  k = int(n/4)\n","  T = np.zeros((n,n))\n","  for j in range(4):\n","    for i in range(k):\n","      if 4*i+8 > 4*k:\n","        T[k*j+i,range((4*i),(4*i+4))] = h[j,range(4)]\n","        T[k*j+i,range(4)] = h[j,range(4,8)]\n","      else:\n","        T[k*j+i,range((4*i),(4*i+8))] = h[j,range(8)]\n","  return T"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y-Chiq7lQIiR"},"source":["def four_Wavelet_Transform(data_path, f):\n","  #Import HMP data\n","  sid_and_class = pd.read_csv(data_path+f+'.csv', index_col=False,\n","                      usecols=['SampleID', 'Race', 'Sex', 'Age', 'BMI', 'SSPG', 'IR_IS_classification'])\n","  hmp = pd.read_csv(data_path+f+'.csv', index_col=False).drop(['SampleID', 'Race', 'Sex', 'Age', 'BMI', 'SSPG', 'IR_IS_classification'], axis=1)\n","\n","  n = hmp.shape[1]\n","  n = n + 4 - (n % 4)\n","  z = np.zeros((hmp.shape[0], n-hmp.shape[1]))\n","  t = fourWTM(n)\n","  s = np.zeros((hmp.shape[0],n))\n","\n","  cols = ['SampleID', 'Race', 'Sex', 'Age', 'BMI', 'SSPG', 'IR_IS_classification'] + list(hmp.columns)\n","\n","  hmp_array = np.concatenate((np.array(hmp), z), axis=1)\n","\n","  ts = np.matmul(hmp_array, t.T)\n","\n","  #Create WT Domain DataFrame\n","  wt_type = 'WT Domain'\n","  df = pd.DataFrame(data=ts)\n","  df = pd.concat([sid_and_class, df], axis=1, ignore_index=True)\n","\n","  for i in range(n - hmp.shape[1]):\n","    cols.append('')\n","  \n","  df.columns = pd.Index(cols)\n","\n","  df.to_csv(data_path+wt_type+'/'+f+'.csv', index=False)\n","\n","  #Create Denoised DataFrame\n","  wt_type = 'Denoised'\n","  dim = int(n/4)\n","  A1 = np.matmul(ts[:, 0:dim], t[0:dim, :])\n","  d = np.zeros((hmp.shape[0],dim,3))\n","  D = np.zeros((hmp.shape[0],n))\n","  for j in range(3):\n","    d[:,:,j] = ts[:, (j+1)*dim:(j+2)*dim]\n","    for k in range(hmp.shape[0]):\n","      #Denoise details\n","      lbda = np.std(d[k,:,j])*math.sqrt(2*math.log(dim))\n","      for i in range(dim):\n","        if abs(d[k,i,j]) < lbda:\n","          d[k,i,j] = 0\n","    D += np.matmul(d[:,:,j], t[(j+1)*dim:(j+2)*dim, :])\n","\n","  s = A1+D\n","  df = pd.DataFrame(data=s)\n","  df = pd.concat([sid_and_class, df], axis=1, ignore_index=True)\n","  df.columns = pd.Index(cols)\n","\n","  df.to_csv(data_path+wt_type+'/'+f+'.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cs1XXj-CZLy6"},"source":["def fourWT_just_wt_domain(data_path, f):\n","  #Import HMP data\n","  sid_and_class = pd.read_csv(data_path+f+'.csv', index_col=False,\n","                      usecols=['SampleID', 'Race', 'Sex', 'Age', 'BMI', 'SSPG', 'IR_IS_classification'])\n","  hmp = pd.read_csv(data_path+f+'.csv', index_col=False).drop(['SampleID', 'Race', 'Sex', 'Age', 'BMI', 'SSPG', 'IR_IS_classification'], axis=1)\n","\n","  n = hmp.shape[1]\n","  n = n + 4 - (n % 4)\n","  z = np.zeros((hmp.shape[0], n-hmp.shape[1]))\n","  t = fourWTM(n)\n","  s = np.zeros((hmp.shape[0],n))\n","\n","  cols = ['SampleID', 'Race', 'Sex', 'Age', 'BMI', 'SSPG', 'IR_IS_classification'] + list(hmp.columns)\n","\n","  hmp_array = np.concatenate((np.array(hmp), z), axis=1)\n","\n","  ts = np.matmul(hmp_array, t.T)\n","\n","  #Create WT Domain DataFrame\n","  wt_type = 'WT Domain'\n","  df = pd.DataFrame(data=ts)\n","  df = pd.concat([sid_and_class, df], axis=1, ignore_index=True)\n","\n","  for i in range(n - hmp.shape[1]):\n","    cols.append('')\n","  \n","  df.columns = pd.Index(cols)\n","\n","  df.to_csv(data_path+wt_type+'/'+f+'.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7e2xjplKZXK2"},"source":["def rna_just_wt_domain():\n","  count = 0\n","  hold_list_of_processes = list()\n","  for j in [preprocessed_data_path, preprocessed_data_path+'Normalized/']:\n","    for i in ['Transcriptomics_VST_excl_3participants','RNAseq_abundance']:\n","      hold_list_of_processes.append(multiprocessing.Process(target=fourWT_just_wt_domain, args=(j, i)))\n","\n","  for p in hold_list_of_processes:\n","    p.start()\n","\n","  for p in hold_list_of_processes:\n","    p.join()\n","    count += 1\n","    print(str(count) + '/' + str(len(hold_list_of_processes)))\n","\n","  print('Done')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7XWp4VHzgtoJ"},"source":["def transform_small_data():\n","  widgets = [' [',\n","        progressbar.Timer(format= 'elapsed time: %(elapsed)s'),\n","        '] ',\n","          progressbar.Bar('#'),' (',\n","          progressbar.ETA(), ') ',\n","          progressbar.Counter(format='%(value)d/%(max_value)d')\n","          ]\n","  bar = progressbar.ProgressBar(max_value=2*(len(hmp_datas)-4), widgets=widgets).start()\n","  count = 0\n","\n","  processes = list()\n","  for j in [preprocessed_data_path, preprocessed_data_path+'Normalized/']:\n","    for i in hmp_datas:\n","      if i not in ['metabolome_abundance', 'proteome_abundance',\n","                  'Transcriptomics_VST_excl_3participants','RNAseq_abundance']:\n","        p = multiprocessing.Process(target=four_Wavelet_Transform, args=(j, i))\n","        processes.append(p)\n","        p.start()\n","\n","  for p in processes:\n","    count += 1\n","    bar.update(count)\n","    p.join()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JpkCvoHHn6rV"},"source":["def four_Wavelet_Transform_big_denoised(data_path, f):\n","  #Import HMP data\n","  sid_and_class = pd.read_csv(data_path+f+'.csv', index_col=False,\n","                      usecols=['SampleID', 'Race', 'Sex', 'Age', 'BMI', 'SSPG', 'IR_IS_classification'])\n","  hmp = pd.read_csv(data_path+f+'.csv', index_col=False).drop(['SampleID', 'Race', 'Sex', 'Age', 'BMI', 'SSPG', 'IR_IS_classification'], axis=1)\n","\n","  n = hmp.shape[1]\n","  n = n + 4 - (n % 4)\n","  z = np.zeros((hmp.shape[0], n-hmp.shape[1]))\n","  t = fourWTM(n)\n","  s = np.zeros((hmp.shape[0],n))\n","\n","  cols = ['SampleID', 'Race', 'Sex', 'Age', 'BMI', 'SSPG', 'IR_IS_classification'] + list(hmp.columns)\n","\n","  hmp_array = np.concatenate((np.array(hmp), z), axis=1)\n","\n","  ts = np.matmul(hmp_array, t.T)\n","\n","  widgets = [' [',\n","        progressbar.Timer(format= 'elapsed time: %(elapsed)s'),\n","        '] ',\n","          progressbar.Bar('#'),' (',\n","          progressbar.ETA(), ') ',\n","          progressbar.Counter(format='%(value)d/%(max_value)d')\n","          ]\n","  bar = progressbar.ProgressBar(max_value=3, widgets=widgets).start()\n","\n","  #Create Denoised DataFrame\n","  wt_type = 'Denoised'\n","  dim = int(n/4)\n","  A1 = np.matmul(ts[:, 0:dim], t[0:dim, :])\n","  d = np.zeros((hmp.shape[0],3*dim))\n","  D = np.zeros((hmp.shape[0],n))\n","\n","  count = 0\n","  for j in range(3):\n","    count += 1\n","    bar.update(count)\n","    d[:, (j)*dim:(j+1)*dim] = ts[:, (j+1)*dim:(j+2)*dim]\n","    for k in range(hmp.shape[0]):\n","      #Denoise details\n","      lbda = np.std(d[k, (j)*dim:(j+1)*dim])*math.sqrt(2*math.log(dim))\n","      for i in range(dim):\n","        if abs(d[k, j*dim+i]) < lbda:\n","          d[k, j*dim+i] = 0\n","  D = np.matmul(d, t[dim:, :])\n","\n","  s = A1+D\n","  df = pd.DataFrame(data=s)\n","  df = pd.concat([sid_and_class, df], axis=1, ignore_index=True)\n","  for i in range(n - hmp.shape[1]):\n","    cols.append('')\n","  df.columns = pd.Index(cols)\n","\n","  df.to_csv(data_path+wt_type+'/'+f+'.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lsIybtrdmToq"},"source":["def transform_big_data():\n","  processes = list()\n","  for j in [preprocessed_data_path, preprocessed_data_path+'Normalized/']:\n","    for i in ['Transcriptomics_VST_excl_3participants','RNAseq_abundance']:\n","      p = multiprocessing.Process(target=four_Wavelet_Transform_big_denoised, args=(j, i))\n","      processes.append(p)\n","      p.start()\n","\n","  for p in processes:\n","    p.join()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g6exmivvmelr","executionInfo":{"status":"ok","timestamp":1631380549223,"user_tz":240,"elapsed":98307,"user":{"displayName":"Adeethyia Shankar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13953621006807715822"}},"outputId":"aaddf8ff-aa68-4858-c9f1-476903630978"},"source":["transform_big_data()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":[" [elapsed time: 0:00:13] |###############################| (ETA:  00:00:00) 3/3"]}]},{"cell_type":"markdown","metadata":{"id":"PwDsauCrg5Xd"},"source":["MAD Dimension Reduction"]},{"cell_type":"code","metadata":{"id":"w1NTDiUHg73e"},"source":["def reduce_dims_mad(data_path, rna_data, threshold):\n","  sid_and_class = pd.read_csv(data_path+rna_data+'.csv', index_col=False,\n","                    usecols=['SampleID', 'Race', 'Sex', 'Age', 'BMI', 'SSPG', 'IR_IS_classification'])\n","  df = pd.read_csv(data_path+rna_data+'.csv', index_col=False).drop(['SampleID', 'Race', 'Sex', 'Age', 'BMI', 'SSPG', 'IR_IS_classification'], axis=1)\n","  cols = df.columns\n","\n","  mad = dict()\n","  for i in df.columns:\n","    mad[i] = stats.median_absolute_deviation(df.loc[:,i], scale=1) / np.median(df.loc[:,i])\n","  mad_df = pd.DataFrame.from_dict(data=mad, orient='index').dropna()\n","  mad_df.columns = pd.Index(['MAD'])\n","  mad_df.sort_values(by='MAD', ascending=False)\n","\n","  count = 1\n","  mad_sum = mad_df.loc[:, 'MAD'].sum()\n","  while mad_df.iloc[0:count, 0].sum() < threshold*mad_sum:\n","    count += 1\n","  \n","  df = pd.concat([sid_and_class, df.iloc[:, 0:count]], axis=1, ignore_index=True)\n","  df.columns = pd.Index(['SampleID', 'Race', 'Sex', 'Age', 'BMI', 'SSPG', 'IR_IS_classification'] + list(cols[0:count]))\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zXgk7zVkvxMX"},"source":["def only_genus():\n","  gut_data = 'gut_16s_abundance'\n","  nares_data = 'nares_16s_abundance'\n","\n","  for data_path in ['', 'WT Domain/', 'Denoised/', 'Normalized/',\n","                    'Normalized/WT Domain/', 'Normalized/Denoised/']:\n","    try:\n","      os.mkdir(preprocessed_data_path+'Genus only/')\n","    except FileExistsError:\n","      pass\n","\n","    for j in [gut_data, nares_data]:\n","      sid_and_class = pd.read_csv(preprocessed_data_path+data_path+j+'.csv', index_col=False,\n","                        usecols=['SampleID', 'Race', 'Sex', 'Age', 'BMI', 'SSPG', 'IR_IS_classification'])\n","      df = pd.read_csv(preprocessed_data_path+data_path+j+'.csv', index_col=False).drop(['SampleID', 'Race', 'Sex', 'Age', 'BMI', 'SSPG', 'IR_IS_classification'], axis=1)\n","      df_cols_to_keep = [i for i in df.columns if i[0:5] == 'genus']\n","      \n","      df = df.loc[:,df_cols_to_keep]\n","      cols = ['SampleID', 'Race', 'Sex', 'Age', 'BMI', 'SSPG', 'IR_IS_classification'] + list(df_cols_to_keep)\n","      df = pd.concat([sid_and_class, df], axis=1, ignore_index=True)\n","      df.columns = pd.Index(cols)\n","      \n","      os.makedirs(preprocessed_data_path+'Genus only/'+data_path, exist_ok=True)\n","      df.to_csv(preprocessed_data_path+'Genus only/'+data_path+j+'.csv', index=False)"],"execution_count":null,"outputs":[]}]}